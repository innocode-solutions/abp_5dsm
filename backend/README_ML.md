# Machine Learning - Servi√ßo Externo

## üèóÔ∏è Arquitetura

O backend agora utiliza um **servi√ßo de ML externo** ao inv√©s de executar modelos Python localmente.

```
backend/
  src/
    service/
      mlService.ts          # Cliente HTTP para o servi√ßo de ML
      predictionService.ts  # Servi√ßo de predi√ß√µes que usa mlService
```

## üîó Configura√ß√£o do Endpoint

O endpoint do servi√ßo de ML √© configurado atrav√©s da vari√°vel de ambiente `ML_SERVICE_URL`:

- **Padr√£o (produ√ß√£o)**: `https://aimodel-teste-deploy.up.railway.app`
- **Desenvolvimento local**: Configure `ML_SERVICE_URL=http://localhost:5000` no seu `.env`

### Vari√°vel de Ambiente

```bash
# No Railway ou arquivo .env
ML_SERVICE_URL=https://aimodel-teste-deploy.up.railway.app
```

## ‚úÖ Verifica√ß√£o

Teste se o servi√ßo de ML est√° funcionando:

```bash
# Health check do ML service (via backend)
curl http://localhost:3000/health/ml

# Health check direto do servi√ßo de ML
curl https://aimodel-teste-deploy.up.railway.app/health
```

## üìù Notas

- O backend faz requisi√ß√µes HTTP para o servi√ßo de ML externo
- O servi√ßo de ML √© um servi√ßo separado (Flask/FastAPI) deployado no Railway
- N√£o √© mais necess√°rio ter Python instalado no backend
- As predi√ß√µes s√£o feitas via API REST: `/predict/dropout` e `/predict/performance`


