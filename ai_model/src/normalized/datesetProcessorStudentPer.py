# =============================================================================
# 1. IMPORTA√á√ïES E CARREGAMENTO DOS DADOS
# =============================================================================
import pandas as pd
import joblib
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Carregar o dataset original
try:
    df = pd.read_csv('datasets/StudentPerformanceFactors.csv')
    print("‚úÖ Dataset carregado com sucesso!")
except FileNotFoundError:
    print("‚ùå Erro: Arquivo 'datasets/StudentPerformanceFactors.csv' n√£o encontrado.")
    # Criando um DataFrame de exemplo para o c√≥digo n√£o quebrar
    data = {
        'Gender': ['Male', 'Female', 'Male', None, 'Female'],
        'Parental_Education_Level': ['High School', 'College', 'Graduate Degree', 'High School', None],
        'Hours_Studied': [3, 5, 2, 6, 4],
        'Exam_Score': [75, 90, 65, 88, 82],
        'Access_to_Resources': ['Yes', 'No', 'Yes', 'Yes', 'No']
    }
    df = pd.DataFrame(data)
    print("‚ÑπÔ∏è Usando um DataFrame de exemplo para demonstra√ß√£o.")


# An√°lise inicial r√°pida
print("\nInforma√ß√µes do DataFrame e valores nulos:")
df.info()


# =============================================================================
# 2. DEFINI√á√ÉO DA ESTRAT√âGIA DE PR√â-PROCESSAMENTO
# =============================================================================
# Identificar colunas por tipo para aplicar as transforma√ß√µes corretas
# (Baseado nas colunas do seu c√≥digo original)
categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()
continuous_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# √â uma boa pr√°tica remover a vari√°vel alvo (target) da lista de features
if 'Exam_Score' in continuous_features:
    continuous_features.remove('Exam_Score')

print("\nIdentifica√ß√£o das Features:")
print("üîπ Features Categ√≥ricas:", categorical_features)
print("üî∏ Features Cont√≠nuas:", continuous_features)


# =============================================================================
# 3. CONSTRU√á√ÉO DO PIPELINE PRINCIPAL ‚öôÔ∏è
# =============================================================================
# Pipeline para features cont√≠nuas:
# Etapa 1: Tratar valores faltantes com a mediana (robusto a outliers)
# Etapa 2: Normalizar os dados (coloc√°-los na mesma escala)
continuous_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Pipeline para features categ√≥ricas:
# Etapa 1: Tratar valores faltantes com o valor mais frequente
# Etapa 2: Aplicar One-Hot Encoding para transformar texto em n√∫meros
categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Combinador: Junta os pipelines e aplica cada um √†s colunas corretas
# √â a nossa "linha de montagem" principal
preprocessor = ColumnTransformer(
    transformers=[
        ('num', continuous_pipeline, continuous_features),
        ('cat', categorical_pipeline, categorical_features)
    ],
    remainder='passthrough' # Mant√©m colunas n√£o especificadas (se houver)
)

print("\n‚úÖ Pipeline de pr√©-processamento constru√≠do!")


# =============================================================================
# 4. APLICA√á√ÉO E SALVAMENTO DO PIPELINE
# =============================================================================
# Separar as features (X) do alvo (y)
X = df.drop(columns='Exam_Score')
y = df['Exam_Score']

# "Treinar" o pipeline de pr√©-processamento com os dados.
# O pipeline aprende as medianas, desvios-padr√£o, categorias, etc.
# E ao mesmo tempo transforma os dados.
print("\nAplicando o pipeline aos dados (fit_transform)...")
X_preprocessed = preprocessor.fit_transform(X)

# O resultado √© um array NumPy. Vamos convert√™-lo de volta para um DataFrame
# para melhor visualiza√ß√£o, usando os nomes de features gerados pelo pipeline.
processed_feature_names = preprocessor.get_feature_names_out()
X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=processed_feature_names)

print("\nResultado ap√≥s o pr√©-processamento (5 primeiras linhas):")


# Salvar o objeto do pipeline "treinado" em um arquivo
output_file = 'perf_preprocess.pkl'
joblib.dump(preprocessor, output_file)

print(f"\n‚úÖ Pipeline final salvo com sucesso em '{output_file}'!")